{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7458f7-7dfd-4f39-a9e1-2bad7180b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large  # Assuming you want to use MobileNetV3Large\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8449d2-0a3d-4241-81b8-719aa5d413e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95276cdb-d6c7-4f6b-a571-9b382d43fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the data\n",
    "DIRECTORY = r'C:\\Users\\aisho\\OneDrive\\Desktop\\data'\n",
    "CATEGORIES = ['fake', 'real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df46547f-1485-48dc-bd1d-0c6caa625ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aisho\\anaconda3\\envs\\tf_new\\lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)  # Preprocessing specific to MobileNetV3\n",
    "        data.append(image)\n",
    "        labels.append(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06da3b4c-26ee-4e64-abdc-6017be4d58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoded format\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3caec2b8-9ada-491a-ae7a-aeb584e97aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data and labels to numpy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf28b2f1-a7b9-4abd-a49e-987031983f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8a5db1-bc98-4e7b-91c5-2187a20ea05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb091b5-5b67-46f5-9b57-9309d1f68e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n",
      "12683000/12683000 [==============================] - 10s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileNetV3 model without the top (fully connected) layers\n",
    "baseModel = MobileNetV3Large(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Construct the head of the model to be placed on top of the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# Place the head FC model on top of the base model\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bf05357-e8d4-4b67-8c45-06162898d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab7a1d7-728d-491a-8e16-012935114bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aisho\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6389 - accuracy: 0.6812WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 41 batches). You may need to use the repeat() function when building your dataset.\n",
      "164/164 [==============================] - 327s 2s/step - loss: 0.6389 - accuracy: 0.6812 - val_loss: 0.4886 - val_accuracy: 0.7854\n",
      "Epoch 2/20\n",
      "164/164 [==============================] - 257s 2s/step - loss: 0.5308 - accuracy: 0.7551\n",
      "Epoch 3/20\n",
      "164/164 [==============================] - 282s 2s/step - loss: 0.4639 - accuracy: 0.7944\n",
      "Epoch 4/20\n",
      "164/164 [==============================] - 248s 2s/step - loss: 0.4246 - accuracy: 0.8242\n",
      "Epoch 5/20\n",
      "164/164 [==============================] - 250s 2s/step - loss: 0.3969 - accuracy: 0.8373\n",
      "Epoch 6/20\n",
      "164/164 [==============================] - 259s 2s/step - loss: 0.3690 - accuracy: 0.8489\n",
      "Epoch 7/20\n",
      "164/164 [==============================] - 336s 2s/step - loss: 0.3437 - accuracy: 0.8625\n",
      "Epoch 8/20\n",
      "164/164 [==============================] - 245s 1s/step - loss: 0.3275 - accuracy: 0.8698\n",
      "Epoch 9/20\n",
      "164/164 [==============================] - 290s 2s/step - loss: 0.3113 - accuracy: 0.8723\n",
      "Epoch 10/20\n",
      "164/164 [==============================] - 199s 1s/step - loss: 0.2906 - accuracy: 0.8834\n",
      "Epoch 11/20\n",
      "164/164 [==============================] - 210s 1s/step - loss: 0.2780 - accuracy: 0.8924\n",
      "Epoch 12/20\n",
      "164/164 [==============================] - 207s 1s/step - loss: 0.2688 - accuracy: 0.8897\n",
      "Epoch 13/20\n",
      "164/164 [==============================] - 200s 1s/step - loss: 0.2627 - accuracy: 0.8930\n",
      "Epoch 14/20\n",
      "164/164 [==============================] - 226s 1s/step - loss: 0.2538 - accuracy: 0.8960\n",
      "Epoch 15/20\n",
      "164/164 [==============================] - 228s 1s/step - loss: 0.2386 - accuracy: 0.9106\n",
      "Epoch 16/20\n",
      "164/164 [==============================] - 216s 1s/step - loss: 0.2322 - accuracy: 0.9140\n",
      "Epoch 17/20\n",
      "164/164 [==============================] - 212s 1s/step - loss: 0.2166 - accuracy: 0.9154\n",
      "Epoch 18/20\n",
      "164/164 [==============================] - 196s 1s/step - loss: 0.2216 - accuracy: 0.9117\n",
      "Epoch 19/20\n",
      "164/164 [==============================] - 172s 1s/step - loss: 0.2119 - accuracy: 0.9232\n",
      "Epoch 20/20\n",
      "164/164 [==============================] - 169s 1s/step - loss: 0.2073 - accuracy: 0.9186\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab6a704-fa9a-45db-a115-d2ff22188e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 34s 724ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "true_labels = np.argmax(testY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30836697-01a9-4588-844c-fd40329901aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.93      0.97      0.95       468\n",
      "        real       0.98      0.96      0.97       846\n",
      "\n",
      "    accuracy                           0.96      1314\n",
      "   macro avg       0.95      0.96      0.96      1314\n",
      "weighted avg       0.96      0.96      0.96      1314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(classification_report(true_labels, predIdxs, target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d382156c-8c50-4f52-b84d-a32083064042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 335ms/step\n",
      "The predicted class is: real\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = r\"C:\\Users\\aisho\\OneDrive\\Desktop\\final_Mohamed_Salah_2018-1710276432640-7179-061fd4d5-d66e-4227-aa3c-90dff174a1cb-8601.jpg\"\n",
    "image = load_img(image_path, target_size=(224, 224))\n",
    "image_array = img_to_array(image)\n",
    "image_array = preprocess_input(image_array.reshape(1, 224, 224, 3))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(image_array)\n",
    "predicted_class_index = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert the predicted index to class label using LabelBinarizer\n",
    "predicted_class_label = lb.classes_[predicted_class_index[0]]\n",
    "\n",
    "print(f\"The predicted class is: {predicted_class_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa61054-9d13-4c15-b3e0-b994a4c60757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history (loss and accuracy)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64bab8-921e-43ac-b78a-1d89f61dc821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predIdxs)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=lb.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c085ff9-f270-49b8-853a-31c969a0c145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945f16c-1be3-44d9-bd22-e93a2c83fce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
